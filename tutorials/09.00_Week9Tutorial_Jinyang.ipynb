{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing using NLTK\n",
    "\n",
    "There are many features of natural languages that are remarkably difficult to get a computer to identify. Much early NLP work was around structuring text according to how it functions as a natural language, not a series of characters as computers read text. Python's NLTK (Natural Language Tool Kit) bundles a bunch of these early tools together for us on text.\n",
    "\n",
    "## Reminder of where we are\n",
    "\n",
    "* values (e.g. `1.2`, `100`, `'Hello, Boston!'`)\n",
    "* value types (e.g., `float`, `int`, `string`)\n",
    "* variables, or objects (for storing and referencing values)\n",
    "* operators (e.g., `=`, `+`, `-`)\n",
    "* logical operators (e.g., `==`, `>`, `<`, `>=`)\n",
    "* statements and expressions (e.g. `10 + 500`)\n",
    "* built-in functions (e.g. `print()`, `type()`)\n",
    "* string functions and string methods (e.g., `string.lower()`, `string.islower()`)\n",
    "* list functions and list metods (e.g., `len(mylist)`, `mylist.append()`)\n",
    "* conditionals (e.g., `if`, `else`, `elif`)\n",
    "* loops (e.g., `for` loops)\n",
    "* user-defined functions (using `def`)\n",
    "* tuples (e.g., ('education', 'high school'))\n",
    "* dictionaries, or key:value pairs\n",
    "* list comprehension - a way to filter lists (and do other things)\n",
    "* Pandas, the dataframe library\n",
    "* Matplotlib, the visualization library\n",
    "* Seaborn, another visualization library that makes everything easier\n",
    "* Today: NLTK, the natural language processing library\n",
    "\n",
    "\n",
    "\n",
    "## Natural Language Processing\n",
    "* *pre-processing*\n",
    "    * Transforming a human lanugage text into computer-manipulable format. A typical pre-processing workflow includes <i>stop-word</i> removal, setting text in lower case, and <i>term frequency</i> counting.\n",
    "* *token*\n",
    "    * An individual word unit within a sentence.\n",
    "* *stop words*\n",
    "    * The function words in a natural langauge, such as <i>the</i>, <i>of</i>, <i>it</i>, etc. These are typically the most common words.\n",
    "* *term frequency*\n",
    "    * The number of times a term appears in a given text. This is either reported as a raw tally or it is <i>normalized</i> by dividing by the total number of words in a text.    \n",
    "* *POS tagging*\n",
    "    * One common task in NLP is the determination of a word's part-of-speech (POS). The label that describes a word's POS is called its <i>tag</i>. Specialized functions that make these determinations are called <i>POS Taggers</i>.\n",
    "* *dependency parsing*\n",
    "    * THe grammatical relationships between words and the type of relationships.\n",
    "* *NLTK (Natural Language Tool Kit)*\n",
    "    * A common Python package that contains many NLP-related functions\n",
    "\n",
    "## Further Resources:\n",
    "\n",
    "Check out the full range of techniques included in Python's nltk package here: http://www.nltk.org/book/\n",
    "\n",
    "As with all of the sections this semester, we could spend an entire semester on NLP alone. This is just meant to give you a taste of what's possible, and equip you with enough knowledge to learn more on your own if you're interested!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jinyang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/jinyang/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jinyang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jinyang/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/jinyang/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jinyang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/jinyang/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import string, which is where we get a list of punctuation\n",
    "import string\n",
    "#First import the Python package nltk (Natural Language Tool Kit)\n",
    "import nltk\n",
    "\n",
    "#NLTK is huge and relies on a bunch of data. These data need to be downloaded. \n",
    "# The two lines download the needed NLTK data to your computer\n",
    "\n",
    "nltk_data = [\"punkt\", \"words\", \"stopwords\", \"averaged_perceptron_tagger\", \n",
    "             \"maxent_ne_chunker\", 'wordnet', 'vader_lexicon']\n",
    "nltk.download(nltk_data)\n",
    "\n",
    "#import the function to split the text into separate words from the NLTK package\n",
    "from nltk import word_tokenize\n",
    "#import the stopwords list\n",
    "from nltk.corpus import stopwords\n",
    "#dependency parser\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For me it has to do with the work that gets done at the crossroads of digital media and traditional humanistic study. And that happens in two different ways. On the one hand, it's bringing the tools and techniques of digital media to bear on traditional humanistic questions; on the other, it's also bringing humanistic modes of inquiry to bear on digital media.\n"
     ]
    }
   ],
   "source": [
    "#Let's go back to our sentence from last week\n",
    "\n",
    "sentence = \"For me it has to do with the work that gets done at the crossroads of \\\n",
    "digital media and traditional humanistic study. And that happens in two different ways. \\\n",
    "On the one hand, it's bringing the tools and techniques of digital media to bear \\\n",
    "on traditional humanistic questions; on the other, it's also bringing humanistic modes \\\n",
    "of inquiry to bear on digital media.\"\n",
    "\n",
    "#print the content\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['For', 'me', 'it', 'has', 'to', 'do', 'with', 'the', 'work', 'that', 'gets', 'done', 'at', 'the', 'crossroads', 'of', 'digital', 'media', 'and', 'traditional', 'humanistic', 'study', '.', 'And', 'that', 'happens', 'in', 'two', 'different', 'ways', '.', 'On', 'the', 'one', 'hand', ',', 'it', \"'s\", 'bringing', 'the', 'tools', 'and', 'techniques', 'of', 'digital', 'media', 'to', 'bear', 'on', 'traditional', 'humanistic', 'questions', ';', 'on', 'the', 'other', ',', 'it', \"'s\", 'also', 'bringing', 'humanistic', 'modes', 'of', 'inquiry', 'to', 'bear', 'on', 'digital', 'media', '.']\n"
     ]
    }
   ],
   "source": [
    "#We tokenized the sentence last week by splitting on the white space.\n",
    "#NLTK has a much more sophisticated approach to tokenizing text\n",
    "#note the difference\n",
    "\n",
    "#create new variable that applies the word_tokenize function to our sentence.\n",
    "sentence_tokens = word_tokenize(sentence)\n",
    "\n",
    "#This new variable contains the tokenized text, and is now a list\n",
    "print(type(sentence_tokens))\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice each token is either a word *or* punctuation - different than splitting on the white space. Careful, does the length indicate word count anymore?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "#The number of tokens is the length of the list, or the number of elements in the list\n",
    "print(len(sentence_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week we used dictionaries and tuples to produce word counts. NLTK has its own function to do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 44 samples and 71 outcomes>\n",
      "[('the', 5), ('it', 3), ('to', 3), ('of', 3), ('digital', 3), ('media', 3), ('humanistic', 3), ('.', 3), ('on', 3), ('that', 2)]\n"
     ]
    }
   ],
   "source": [
    "#apply the nltk function FreqDist to count the number of times each token occurs.\n",
    "word_frequency = nltk.FreqDist(sentence_tokens)\n",
    "\n",
    "print(word_frequency)\n",
    "\n",
    "#print out the 10 most frequent words using the function most_common\n",
    "print(word_frequency.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we still need to do pre-processing.\n",
    "\n",
    "## Pre-Processing: Lower Case, Removing Stop Words and Punctuation\n",
    "\n",
    "\n",
    "To convert to lower case we use the function lower() and list comprehension. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'me', 'it', 'has', 'to', 'do', 'with', 'the', 'work', 'that', 'gets', 'done', 'at', 'the', 'crossroads', 'of', 'digital', 'media', 'and', 'traditional', 'humanistic', 'study', '.', 'and', 'that', 'happens', 'in', 'two', 'different', 'ways', '.', 'on', 'the', 'one', 'hand', ',', 'it', \"'s\", 'bringing', 'the', 'tools', 'and', 'techniques', 'of', 'digital', 'media', 'to', 'bear', 'on', 'traditional', 'humanistic', 'questions', ';', 'on', 'the', 'other', ',', 'it', \"'s\", 'also', 'bringing', 'humanistic', 'modes', 'of', 'inquiry', 'to', 'bear', 'on', 'digital', 'media', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens_lc = [word.lower() for word in sentence_tokens]\n",
    "\n",
    "#see the result\n",
    "print(sentence_tokens_lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words like \"the\", \"to\", and \"and\" are what text analysis call \"stop words.\" Stop words are the most common words in a language, and while necessary and useful for some analysis purposes, do not tell us much about the *substance* of a text. Another common pre-processing steps is to simply remove punctuation and stop words. NLTK contains a built-in stop words list, which we use to remove stop words from our list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#take a look at what stop words are included:\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work', 'gets', 'done', 'crossroads', 'digital', 'media', 'traditional', 'humanistic', 'study', '.', 'happens', 'two', 'different', 'ways', '.', 'one', 'hand', ',', \"'s\", 'bringing', 'tools', 'techniques', 'digital', 'media', 'bear', 'traditional', 'humanistic', 'questions', ';', ',', \"'s\", 'also', 'bringing', 'humanistic', 'modes', 'inquiry', 'bear', 'digital', 'media', '.']\n"
     ]
    }
   ],
   "source": [
    "#create a new variable that contains the sentence tokens without the stopwords\n",
    "sentence_tokens_clean = [word for word in sentence_tokens_lc if word not in stopwords.words('english')]\n",
    "\n",
    "#see what words we're left with\n",
    "print(sentence_tokens_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation also does not help us understand the substance of a text, so we'll remove punctuation in a similar fashion. [Again, think about tasks where me may not want to remove punctuation.] There are many many ways to do this. For now, we'll create a list of punctuation tokens, similar to the list of stop words, and remove them from our list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "#creat list of punctuation symbols\n",
    "punctuation = list(string.punctuation)\n",
    "\n",
    "#see what punctuation is included\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work', 'gets', 'done', 'crossroads', 'digital', 'media', 'traditional', 'humanistic', 'study', 'happens', 'two', 'different', 'ways', 'one', 'hand', \"'s\", 'bringing', 'tools', 'techniques', 'digital', 'media', 'bear', 'traditional', 'humanistic', 'questions', \"'s\", 'also', 'bringing', 'humanistic', 'modes', 'inquiry', 'bear', 'digital', 'media']\n"
     ]
    }
   ],
   "source": [
    "#remove punctuation from our tokens\n",
    "sentence_tokens_clean = [w for w in sentence_tokens_clean if w not in punctuation]\n",
    "\n",
    "#see what's left\n",
    "print(sentence_tokens_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after our pre-processing steps, let's re-count the most frequent words in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('digital', 3), ('media', 3), ('humanistic', 3), ('traditional', 2), (\"'s\", 2), ('bringing', 2), ('bear', 2), ('work', 1), ('gets', 1), ('done', 1)]\n"
     ]
    }
   ],
   "source": [
    "word_frequency_clean = nltk.FreqDist(sentence_tokens_clean)\n",
    "\n",
    "print(word_frequency_clean.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we replicated what we did last week! In slightly fewer steps, but in a more sophisticated way.\n",
    "\n",
    "NLTK can do so much more.\n",
    "\n",
    "\n",
    "## Part-of-Speech Tagging\n",
    "\n",
    "You may have noticed that stop words are typically short function words. Intuitively, if we could identify the part of speech of a word, we would have another way of identifying words of substance. NLTK can do that too!\n",
    "\n",
    "NLTK has a function that will tag the part of speech of every token in a text. For this, we go back to our original tokenized text, with the stop words and punctuation.\n",
    "\n",
    "NLTK uses the Penn Treebank Project to tag the part-of-speech of the words. You can find a list of all the part-of-speech tags here:\n",
    "\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('For', 'IN'), ('me', 'PRP'), ('it', 'PRP'), ('has', 'VBZ'), ('to', 'TO'), ('do', 'VB'), ('with', 'IN'), ('the', 'DT'), ('work', 'NN'), ('that', 'WDT'), ('gets', 'VBZ'), ('done', 'VBN'), ('at', 'IN'), ('the', 'DT'), ('crossroads', 'NNS'), ('of', 'IN'), ('digital', 'JJ'), ('media', 'NNS'), ('and', 'CC'), ('traditional', 'JJ'), ('humanistic', 'JJ'), ('study', 'NN'), ('.', '.'), ('And', 'CC'), ('that', 'DT'), ('happens', 'VBZ'), ('in', 'IN'), ('two', 'CD'), ('different', 'JJ'), ('ways', 'NNS'), ('.', '.'), ('On', 'IN'), ('the', 'DT'), ('one', 'CD'), ('hand', 'NN'), (',', ','), ('it', 'PRP'), (\"'s\", 'VBZ'), ('bringing', 'VBG'), ('the', 'DT'), ('tools', 'NNS'), ('and', 'CC'), ('techniques', 'NNS'), ('of', 'IN'), ('digital', 'JJ'), ('media', 'NNS'), ('to', 'TO'), ('bear', 'VB'), ('on', 'IN'), ('traditional', 'JJ'), ('humanistic', 'JJ'), ('questions', 'NNS'), (';', ':'), ('on', 'IN'), ('the', 'DT'), ('other', 'JJ'), (',', ','), ('it', 'PRP'), (\"'s\", 'VBZ'), ('also', 'RB'), ('bringing', 'VBG'), ('humanistic', 'JJ'), ('modes', 'NNS'), ('of', 'IN'), ('inquiry', 'NN'), ('to', 'TO'), ('bear', 'VB'), ('on', 'IN'), ('digital', 'JJ'), ('media', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#use the nltk pos function to tag the tokens\n",
    "tagged_sentence_tokens = nltk.pos_tag(sentence_tokens)\n",
    "\n",
    "#view new variable\n",
    "print(tagged_sentence_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check your variable type!\n",
    "\n",
    "type(tagged_sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It's, of course, a list of tuples\n",
    "\n",
    "type(tagged_sentence_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count the part-of-speech tags in a similar way we counted words, to output the most frequent types of words in our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IN', 'PRP', 'PRP', 'VBZ', 'TO', 'VB', 'IN', 'DT', 'NN', 'WDT', 'VBZ', 'VBN', 'IN', 'DT', 'NNS', 'IN', 'JJ', 'NNS', 'CC', 'JJ', 'JJ', 'NN', '.', 'CC', 'DT', 'VBZ', 'IN', 'CD', 'JJ', 'NNS', '.', 'IN', 'DT', 'CD', 'NN', ',', 'PRP', 'VBZ', 'VBG', 'DT', 'NNS', 'CC', 'NNS', 'IN', 'JJ', 'NNS', 'TO', 'VB', 'IN', 'JJ', 'JJ', 'NNS', ':', 'IN', 'DT', 'JJ', ',', 'PRP', 'VBZ', 'RB', 'VBG', 'JJ', 'NNS', 'IN', 'NN', 'TO', 'VB', 'IN', 'JJ', 'NNS', '.']\n"
     ]
    }
   ],
   "source": [
    "#Check your understanding of the line below, \n",
    "# it's a form of list comprehension to translate a list of tuples into a list\n",
    "print([tag for (word, tag) in tagged_sentence_tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('IN', 11),\n",
       " ('JJ', 10),\n",
       " ('NNS', 9),\n",
       " ('DT', 6),\n",
       " ('VBZ', 5),\n",
       " ('PRP', 4),\n",
       " ('NN', 4),\n",
       " ('TO', 3),\n",
       " ('VB', 3),\n",
       " ('CC', 3)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can do a frequency distribution on a list of strings\n",
    "tagged_frequency = nltk.FreqDist([tag for (word, tag) in tagged_sentence_tokens])\n",
    "\n",
    "tagged_frequency.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sentence contains a lot of adjectives. So let's first look at the most frequent adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['digital', 'traditional', 'humanistic', 'different', 'digital', 'traditional', 'humanistic', 'other', 'humanistic', 'digital']\n"
     ]
    }
   ],
   "source": [
    "adjectives = [word for word,pos in tagged_sentence_tokens if pos == 'JJ' or pos=='JJR' or pos=='JJS']\n",
    "\n",
    "#print all of the adjectives\n",
    "print(adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('digital', 3), ('humanistic', 3), ('traditional', 2), ('different', 1), ('other', 1)]\n"
     ]
    }
   ],
   "source": [
    "#calculate the frequency of the adjectives\n",
    "freq_adjectives=nltk.FreqDist(adjectives)\n",
    "\n",
    "#print the most frequent adjectives\n",
    "print(freq_adjectives.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work', 'crossroads', 'media', 'study', 'ways', 'hand', 'tools', 'techniques', 'media', 'questions', 'modes', 'inquiry', 'media']\n"
     ]
    }
   ],
   "source": [
    "nouns = [word for (word,pos) in tagged_sentence_tokens if pos=='NN' or pos=='NNS']\n",
    "\n",
    "#print all of the nouns\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('media', 3), ('work', 1), ('crossroads', 1), ('study', 1), ('ways', 1), ('hand', 1), ('tools', 1), ('techniques', 1), ('questions', 1), ('modes', 1)]\n"
     ]
    }
   ],
   "source": [
    "#calculate the frequency of the nouns\n",
    "freq_nouns=nltk.FreqDist(nouns)\n",
    "\n",
    "#print the most frequent nouns\n",
    "print(freq_nouns.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['has', 'do', 'gets', 'done', 'happens', \"'s\", 'bringing', 'bear', \"'s\", 'bringing', 'bear']\n"
     ]
    }
   ],
   "source": [
    "verbs = [word for word,pos in tagged_sentence_tokens if pos == 'VB' or pos=='VBD' or pos=='VBG' or pos=='VBN' or pos=='VBP' or pos=='VBZ']\n",
    "\n",
    "#print all of the verbs\n",
    "print(verbs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"'s\", 2), ('bringing', 2), ('bear', 2), ('has', 1), ('do', 1), ('gets', 1), ('done', 1), ('happens', 1)]\n"
     ]
    }
   ],
   "source": [
    "#calculate the frequency of the verbs\n",
    "freq_verbs=nltk.FreqDist(verbs)\n",
    "\n",
    "#print the most frequent verbs\n",
    "print(freq_verbs.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we bring all of this together we get a pretty good summary of the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('digital', 3), ('humanistic', 3), ('traditional', 2)]\n",
      "[('media', 3), ('work', 1), ('crossroads', 1)]\n",
      "[(\"'s\", 2), ('bringing', 2), ('bear', 2)]\n"
     ]
    }
   ],
   "source": [
    "print(freq_adjectives.most_common(3))\n",
    "print(freq_nouns.most_common(3))\n",
    "print(freq_verbs.most_common(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration: Compare Melville to Austen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to output the most frequent words based on a part of speech\n",
    "import string\n",
    "\n",
    "def read_file(filename):\n",
    "        \n",
    "    with open(filename, 'r', encoding='utf-8') as myfile:\n",
    "        mytext = myfile.read()\n",
    "    return(mytext)\n",
    "\n",
    "def freq_words_pos(filename, pos_tag_list):\n",
    "    \"\"\"\n",
    "    This is called a docstring. It defines and explains what the function is doing.\n",
    "    Any function more than a few lines should include a docstring\n",
    "    \n",
    "    This function takes a filename containing text and a list of part of speeches,\n",
    "    and outputs the most frequent words for that part of speech\n",
    "    \n",
    "    Input: filename and path, list of penn treebank part of speech tags\n",
    "    Output: List of tuples words and counts for the pos, in descending order\n",
    "    \"\"\"\n",
    "    \n",
    "    mytext = read_file(filename) #calling a function inside a function!\n",
    "    \n",
    "    punctuation = list(string.punctuation)\n",
    "        \n",
    "    tokens = word_tokenize(mytext)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    \n",
    "    freq_words = [word for word,pos in tagged if pos in pos_tag_list]\n",
    "    \n",
    "    return(nltk.FreqDist(freq_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('whale', 714),\n",
       " ('man', 472),\n",
       " ('ship', 440),\n",
       " ('sea', 346),\n",
       " ('time', 314),\n",
       " ('boat', 280),\n",
       " ('head', 262),\n",
       " ('way', 256),\n",
       " ('whales', 231),\n",
       " ('men', 228),\n",
       " ('hand', 196),\n",
       " ('thing', 184),\n",
       " ('side', 176),\n",
       " ('ye', 169),\n",
       " ('world', 167),\n",
       " ('water', 162),\n",
       " ('day', 157),\n",
       " ('deck', 157),\n",
       " ('eyes', 154),\n",
       " ('sort', 151)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_words_pos('../data/Melville_MobyDick.txt', ['NN', 'NNS']).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sister', 266),\n",
       " ('mother', 248),\n",
       " ('time', 235),\n",
       " ('thing', 182),\n",
       " ('nothing', 163),\n",
       " ('house', 146),\n",
       " ('day', 143),\n",
       " ('heart', 126),\n",
       " ('man', 118),\n",
       " ('moment', 97),\n",
       " ('room', 97),\n",
       " ('mind', 95),\n",
       " ('kind', 94),\n",
       " ('world', 90),\n",
       " ('morning', 85),\n",
       " ('town', 85),\n",
       " ('family', 81),\n",
       " ('affection', 79),\n",
       " ('brother', 78),\n",
       " ('place', 76)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_words_pos('../data/Austen_SenseAndSensibility.txt', ['NN', 'NNS']).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "We can also tag named entities - person, places, organizations, etc. Similar syntax, just using the `ne_chunk` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Google/NNP)\n",
      "  moved/VBD\n",
      "  their/PRP$\n",
      "  headquarters/NNS\n",
      "  from/IN\n",
      "  (GPE San/NNP)\n",
      "  Jose/NNP\n",
      "  to/TO\n",
      "  (GPE Seattle/NNP)\n",
      "  ,/,\n",
      "  per/IN\n",
      "  spokesperson/NN\n",
      "  (PERSON Sudhir/NNP))\n"
     ]
    }
   ],
   "source": [
    "#tokenize our text\n",
    "ner_tokens = word_tokenize('Google moved their headquarters from San Jose to Seattle, per spokesperson Sudhir')\n",
    "\n",
    "#tag it with part-of-speech\n",
    "ner_tokens_tagged = nltk.pos_tag(ner_tokens)\n",
    "\n",
    "#add a named entity tage\n",
    "namedEnt = nltk.ne_chunk(ner_tokens_tagged)\n",
    "print(namedEnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing\n",
    "\n",
    "Dependency parsing finds grammatical relationships between words and the type of relationships. In the social sciences, this is typically focused on subject-verb-object relationships (who does what to whom).\n",
    "\n",
    "You can train your own model here, but we will be using [Stanford's CoreNLP dependency parser](https://nlp.stanford.edu/software/stanford-dependencies.html). It's fast relative to other dependency parsers, but as you might learn if you try to do this on a longer text, it's also quite slow. \n",
    "\n",
    "Because depdency parser parse relationships among words, it can produce a lot of output. We'll try it on a simple sentence.\n",
    "\n",
    "Stanford dependencies are triplets: name of the relation, governor and dependent.You can find a pdf that includes the definition of the Stanford typed relationships [here](https://downloads.cs.stanford.edu/nlp/software/dependencies_manual.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zj/0n68ddwj13s62557809lw1200000gn/T/ipykernel_39717/1929504786.py:7: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('shot', 'VBD'), 'nsubj', ('I', 'PRP')),\n",
       " (('shot', 'VBD'), 'obj', ('elephant', 'NN')),\n",
       " (('elephant', 'NN'), 'det', ('an', 'DT')),\n",
       " (('shot', 'VBD'), 'obl', ('sleep', 'NN')),\n",
       " (('sleep', 'NN'), 'case', ('in', 'IN')),\n",
       " (('sleep', 'NN'), 'nmod:poss', ('my', 'PRP$'))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#point to the Stanford model and software\n",
    "\n",
    "path_to_jar = '../stanford_parser/stanford-corenlp-4.2.2/stanford-corenlp-4.2.2.jar'\n",
    "path_to_models_jar = '../stanford_parser/stanford-corenlp-4.2.2-models-english.jar'\n",
    "\n",
    "#use the Python NLTK wrapper to implement it\n",
    "dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "#parse a simple sentence\n",
    "result = dependency_parser.raw_parse('I shot an elephant in my sleep')\n",
    "\n",
    "#access the output\n",
    "dep = result.__next__()\n",
    "\n",
    "#print the relationships\n",
    "list(dep.triples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check your variable types!\n",
    "print(type(dep.triples()))\n",
    "type(list(dep.triples()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('shot', 'VBD'), 'nsubj', ('I', 'PRP'))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at the first element\n",
    "list(dep.triples())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check your variale type!\n",
    "type(list(dep.triples())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Govenor:\n",
      "('shot', 'VBD')\n",
      "Relationship\n",
      "nsubj\n",
      "Dependent\n",
      "('I', 'PRP')\n",
      "Govenor:\n",
      "('shot', 'VBD')\n",
      "Relationship\n",
      "obj\n",
      "Dependent\n",
      "('elephant', 'NN')\n",
      "Govenor:\n",
      "('elephant', 'NN')\n",
      "Relationship\n",
      "det\n",
      "Dependent\n",
      "('an', 'DT')\n",
      "Govenor:\n",
      "('shot', 'VBD')\n",
      "Relationship\n",
      "obl\n",
      "Dependent\n",
      "('sleep', 'NN')\n",
      "Govenor:\n",
      "('sleep', 'NN')\n",
      "Relationship\n",
      "case\n",
      "Dependent\n",
      "('in', 'IN')\n",
      "Govenor:\n",
      "('sleep', 'NN')\n",
      "Relationship\n",
      "nmod:poss\n",
      "Dependent\n",
      "('my', 'PRP$')\n",
      "\n",
      "All nsubj relationships:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('shot', 'VBD'), 'nsubj', ('I', 'PRP'))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ah, the tuple. Let's find all the 'nsubj' dependencies (there's only one, but imagine with me)\n",
    "\n",
    "nsubj = []\n",
    "\n",
    "for gov, rel, depend in list(dep.triples()):\n",
    "    print(\"Govenor:\")\n",
    "    print(gov)\n",
    "    print(\"Relationship\")\n",
    "    print(rel)\n",
    "    print(\"Dependent\")\n",
    "    print(depend)\n",
    "    if rel == 'nsubj':\n",
    "        nsubj.append((gov, rel, depend))\n",
    "\n",
    "print()\n",
    "print(\"All nsubj relationships:\")\n",
    "nsubj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis!\n",
    "\n",
    "NLTK has a sentiment analyzer took called Vader. Vader is not great, no sentiment tools is, but it's OK, and works well on more contemporary text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis using Vader\n",
    "\n",
    "You can find more information on Vader and what it can do [here](https://towardsdatascience.com/sentimental-analysis-using-vader-a3415fef7664).\n",
    "\n",
    "VADER’s SentimentIntensityAnalyzer() takes in a string and returns a `dictionary` of scores in each of four categories:\n",
    "\n",
    "* negative\n",
    "* neutral\n",
    "* positive\n",
    "* compound (computed by normalizing the scores above)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.951, 'pos': 0.049, 'compound': 0.4939}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#assign the analyzer function to its own object (yes, an object can be a function!)\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "#Let's run it on our original sentence variable\n",
    "sid.polarity_scores(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.049"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It's a dictionary, so we can pull out the values separately\n",
    "\n",
    "sid.polarity_scores(sentence)['pos']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises!\n",
    "\n",
    "Whew, that was a lot, but take a few moments to practice. Then practice more over the week!\n",
    "\n",
    "1. Print the most frequent adjective in Moby Dick and Sense and Sensibility. Does anything intrigue you?\n",
    "    * Hint, remind yourself of the tags [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n",
    "2. Print the most frequent verbs in Moby Dick and Sense and Sensibility. Does anything intrigue you?\n",
    "3. Who is more negative? Melville or Austen? Who is more positive? Before you calculate, think through a hypothesis (if you know these authors).\n",
    "    * Hint: Vader works better on short texts. You probaby want to calculate the sentiment for each sentence separately, and then take the average. NLTK has a sentence tokenizer! See if you can use the documentation to get it to work.\n",
    "4. Parse a few sentences from either Melville or Austen:\n",
    "    * Identify all nsubj or obj relationships. Anything interesting?\n",
    "    * Extract named entities. Anything interesting?\n",
    "        * Hint: don't try the full novel, it'll take too long and be far too much output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Print the most frequent adjective in Moby Dick and Sense and Sensibility. Does anything intrigue you?  \n",
    "Hint, remind yourself of the tags here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_words_pos(filename, pos_tag_list):\n",
    "    \"\"\"\n",
    "    This is called a docstring. It defines and explains what the function is doing.\n",
    "    Any function more than a few lines should include a docstring\n",
    "    \n",
    "    This function takes a filename containing text and a list of part of speeches,\n",
    "    and outputs the most frequent words for that part of speech\n",
    "    \n",
    "    Input: filename and path, list of penn treebank part of speech tags\n",
    "    Output: List of tuples words and counts for the pos, in descending order\n",
    "    \"\"\"\n",
    "    \n",
    "    mytext = read_file(filename) #calling a function inside a function!\n",
    "    \n",
    "    punctuation = list(string.punctuation)\n",
    "        \n",
    "    tokens = word_tokenize(mytext)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    \n",
    "    freq_words = [word for word,pos in tagged if pos in pos_tag_list]\n",
    "    \n",
    "    return(nltk.FreqDist(freq_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('old', 427),\n",
       " ('other', 409),\n",
       " ('great', 290),\n",
       " ('last', 273),\n",
       " ('such', 256),\n",
       " ('more', 245),\n",
       " ('little', 238),\n",
       " ('same', 209),\n",
       " ('own', 201),\n",
       " ('long', 187),\n",
       " ('first', 174),\n",
       " ('good', 172),\n",
       " ('many', 160),\n",
       " ('white', 158),\n",
       " ('much', 134),\n",
       " ('small', 121),\n",
       " ('whole', 115),\n",
       " ('full', 110),\n",
       " ('poor', 99),\n",
       " ('thy', 93)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_words_pos('../data/Melville_MobyDick.txt', ['JJ', 'JJR','JJS']).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('own', 267),\n",
       " ('such', 259),\n",
       " ('more', 184),\n",
       " ('other', 182),\n",
       " ('much', 170),\n",
       " ('little', 148),\n",
       " ('great', 147),\n",
       " ('good', 131),\n",
       " ('first', 121),\n",
       " ('sure', 118),\n",
       " ('young', 103),\n",
       " ('last', 100),\n",
       " ('same', 100),\n",
       " ('many', 97),\n",
       " ('happy', 94),\n",
       " ('present', 77),\n",
       " ('few', 77),\n",
       " ('dear', 73),\n",
       " ('least', 64),\n",
       " ('better', 57)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_words_pos('../data/Austen_SenseAndSensibility.txt', ['JJ','JJR', 'JJS']).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 1024),\n",
       " ('have', 353),\n",
       " ('see', 164),\n",
       " ('go', 125),\n",
       " ('do', 110),\n",
       " ('let', 100),\n",
       " ('say', 87),\n",
       " ('make', 86),\n",
       " ('take', 83),\n",
       " ('get', 79),\n",
       " ('tell', 79),\n",
       " ('ye', 69),\n",
       " ('come', 67),\n",
       " ('know', 67),\n",
       " ('think', 57),\n",
       " ('keep', 57),\n",
       " ('give', 54),\n",
       " ('till', 52),\n",
       " ('seem', 50),\n",
       " ('look', 43)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_words_pos('../data/Melville_MobyDick.txt', ['VB']).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 1289),\n",
       " ('have', 451),\n",
       " ('do', 152),\n",
       " ('see', 140),\n",
       " ('make', 132),\n",
       " ('say', 128),\n",
       " ('give', 122),\n",
       " ('think', 122),\n",
       " ('know', 90),\n",
       " ('go', 89),\n",
       " ('tell', 68),\n",
       " ('hear', 59),\n",
       " ('come', 55),\n",
       " ('speak', 54),\n",
       " ('find', 52),\n",
       " ('feel', 51),\n",
       " ('leave', 42),\n",
       " ('take', 41),\n",
       " ('believe', 38),\n",
       " ('till', 36)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_words_pos('../data/Austen_SenseAndSensibility.txt', ['VB']).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "austen=read_file('../data/Austen_SenseAndSensibility.txt')\n",
    "meville=read_file('../data/Melville_MobyDick.txt')\n",
    "austen_sent = sent_tokenize(austen)\n",
    "meville_sent= sent_tokenize(meville)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "austen_pos=[]\n",
    "for sent in austen_sent:\n",
    "    austen_pos.append(sid.polarity_scores(sent)['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meville_pos = [sid.polarity_scores(sent)['pos'] for sent in meville_sent]\n",
    "\n",
    "austen_neg = [sid.polarity_scores(sent)['neg'] for sent in austen_sent]\n",
    "meville_neg = [sid.polarity_scores(sent)['neg'] for sent in meville_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(austen_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austen_pos[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
