{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Analysis, with a side of Dictionaries\n",
    "\n",
    "Now that we've learned some Python basics, we'll move to applying our tools to do analyses beyond graphing numbers. Our primary example: text analysis.\n",
    "\n",
    "To do so, we often don't want lists or dataframes, but we want certain elements to be associated with values (we covered this a bit last week). A person will have an income, for example, or a novel will use the word `humanistic` a certain number of times. Today we'll think through data types that can help us we these associations. This form of data is called linked data.\n",
    "\n",
    "\n",
    "# Tuples, Dictionaries, and List and Dictionary Comprehension\n",
    "\n",
    "A *tuple* is a collection of objects which is ordered and unchangeable. In Python tuples are written with round brackets.\n",
    "\n",
    "A *dictionary* in Python is an unordered collection of data values, used to store data values like a map, which unlike other Data Types that hold only single value as an element, a *dictionary* holds key:value pairs.\n",
    "\n",
    "Like lists, dictionaries can easily be changed, can be shrunk and grown ad libitum at run time. They shrink and grow without the necessity of making copies. Dictionaries can be contained in lists and vice versa. \n",
    "\n",
    "But what's the difference between lists and dictionaries? Lists are ordered sets of objects, whereas dictionaries are unordered sets. But the main difference is that items in dictionaries are accessed via keys and not via their position. A dictionary is an associative array (also known as hashes). Any key of the dictionary is associated (or mapped) to a value. The values of a dictionary can be any Python data type. So dictionaries are unordered key-value-pairs. \n",
    "\n",
    "*List comprehension* is a syntactic construct available in some programming languages for creating a list based on existing lists. It condenses what we did before, looping through lists, down to one line. (It's way more powerful, but we'll just get a taste here.)\n",
    "\n",
    "\n",
    "# Self-Defined Functions\n",
    "\n",
    "So far, we have only been using the functions that come with Python, but it is also possible to add new functions. A function definition specifies the name of a new function and the sequence of statements that execute when the function is called. Once we define a function, we can reuse the function over and over throughout our program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining your own function\n",
    "\n",
    "\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_lyrics():\n",
    "    print(\"I'm a lumberjack, and I'm okay.\")\n",
    "    print('I sleep all night and I work all day.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`def` is a keyword that indicates that this is a function definition. The name of the function is print_lyrics. The rules for function names are the same as for variable names: letters, numbers and some punctuation marks are legal, but the first character can't be a number. You can't use a keyword as the name of a function, and you should avoid having a variable and a function with the same name.\n",
    "\n",
    "The empty parentheses after the name indicate that this function doesn't take any arguments. Later we will build functions that take arguments as their inputs.\n",
    "\n",
    "The first line of the function definition is called the *header*; the rest is called the *body*. The header has to end with a colon and the body has to be indented. By convention, the indentation is always four spaces. The body can contain any number of statements.\n",
    "\n",
    "The strings in the print statements are enclosed in quotes. Single quotes and double quotes do the same thing; most people use single quotes except in cases like this where a single quote (which is also an apostrophe) appears in the string.\n",
    "\n",
    "The syntax for calling the new function is the same as for built-in functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a lumberjack, and I'm okay.\n",
      "I sleep all night and I work all day.\n"
     ]
    }
   ],
   "source": [
    "print_lyrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have defined a function, you can use it inside another function. For example, to repeat the previous refrain, we could write a function called `repeat_lyrics`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_lyrics():\n",
    "    print_lyrics()\n",
    "    print_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a lumberjack, and I'm okay.\n",
      "I sleep all night and I work all day.\n",
      "I'm a lumberjack, and I'm okay.\n",
      "I sleep all night and I work all day.\n"
     ]
    }
   ],
   "source": [
    "repeat_lyrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters and Arugments\n",
    "\n",
    "Some of the built-in functions we have seen require arguments.\n",
    "\n",
    "Inside the function, the arguments are assigned to variables called parameters. Here is an example of a user-defined function that takes an argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_length(p):\n",
    "    print(len(p))\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function assigns the argument to a parameter named phrase. When the function is called, it prints length of the value of the parameter (whatever it is).\n",
    "\n",
    "This function works with any value that can be an argument for the length function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "In the beginning\n",
      "15\n",
      "Call me Ishmael\n",
      "516\n",
      "I heard a Fly buzz – when I died –  The Stillness in the Room Was like the Stillness in the Air –   Between the Heaves of Storm – The Eyes around – had wrung them dry –  And Breaths were gathering firm For that last Onset – when the King Be witnessed – in the Room –  I willed my Keepsakes – Signed away What portions of me be Assignable – and then it was There interposed a Fly –  With Blue – uncertain stumbling Buzz –  Between the light – and me –  And then the Windows failed – and then I could not see to see – \n"
     ]
    }
   ],
   "source": [
    "phrase_length(\"In the beginning\")\n",
    "phrase_length(\"Call me Ishmael\")\n",
    "phrase_length(\"I heard a Fly buzz – when I died –  \\\n",
    "The Stillness in the Room \\\n",
    "Was like the Stillness in the Air –   \\\n",
    "Between the Heaves of Storm – \\\n",
    "\\\n",
    "The Eyes around – had wrung them dry –  \\\n",
    "And Breaths were gathering firm \\\n",
    "For that last Onset – when the King \\\n",
    "Be witnessed – in the Room –  \\\n",
    "\\\n",
    "I willed my Keepsakes – Signed away \\\n",
    "What portions of me be \\\n",
    "Assignable – and then it was \\\n",
    "There interposed a Fly –  \\\n",
    "\\\n",
    "With Blue – uncertain stumbling Buzz –  \\\n",
    "Between the light – and me –  \\\n",
    "And then the Windows failed – and then \\\n",
    "I could not see to see – \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Functions?\n",
    "\n",
    "It may not be clear why it is worth the trouble to divide a program into functions. There are several reasons:\n",
    "\n",
    "* Creating a new function gives you an opportunity to name a group of statements, which makes your program easier to read, understand, and debug.\n",
    "\n",
    "* Functions can make a program smaller by eliminating repetitive code. Later, if you make a change, you only have to make it in one place.\n",
    "\n",
    "* Dividing a long program into functions allows you to debug the parts one at a time and then assemble them into a working whole.\n",
    "\n",
    "* Well-designed functions are often useful for many programs. Once you write and debug one, you can reuse it.\n",
    "\n",
    "Throughout the rest of the course, often we will use a function definition to explain a concept. Part of the skill of creating and using functions is to have a function properly capture an idea such as \"find the smallest value in a list of values\". Later we will show you code that finds the smallest in a list of values and we will present it to you as a function named min which takes a list of values as its argument and returns the smallest value in the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuples and Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "my_tuple = [('education', 'high school'), ('income', 100)]\n",
    "\n",
    "print(type(my_tuple))\n",
    "print(type(my_tuple[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key is: \n",
      "education\n",
      "The value is: \n",
      "high school\n",
      "\n",
      "\n",
      "The key is: \n",
      "income\n",
      "The value is: \n",
      "100\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#You can loop through tuples, but you need to assign multiple variables when you loop through them:\n",
    "for key, value in my_tuple:\n",
    "    print(\"The key is: \")\n",
    "    print(key)\n",
    "    print(\"The value is: \")\n",
    "    print(value)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict = dict(my_tuple)\n",
    "type(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'education': 'high school', 'income': 100}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key is before the colon, the value is after the colon. \n",
    "\n",
    "Find all the keys from the dictionary, and then all the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['education', 'income'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['high school', 100])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access keys using the bracket syntax. We've seen this before (remember columns in Pandas?). The input is a dictionary key, the output is the key's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'high school'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict['education']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict['income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add key/value pairs using the bracket syntax and the assignment operator. Notice the order of the key/value pairs does not matter, like they do in lists and strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'education': 'high school', 'income': 100, 'age': 24}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict['age'] = 24\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'there']\n"
     ]
    }
   ],
   "source": [
    "#list comprehension!\n",
    "\n",
    "mylist = ['In', 'the', 'beginning', 'there', 'was', 'chaos']\n",
    "\n",
    "# Use a loop to filter out words that begin with 't':\n",
    "\n",
    "t_words = []\n",
    "\n",
    "for word in mylist:\n",
    "    if word.lower().startswith('t'):\n",
    "        t_words.append(word)\n",
    "\n",
    "print(t_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'there']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now do it with list comprehension\n",
    "\n",
    "t_words = [word for word in mylist if word.lower().startswith('t')]\n",
    "t_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Counting Words\n",
    "\n",
    "We have been looking at different features of \"words\" (or, as Python knows them, elements in a string separated by white space). What if we want to find the number of times each word occurs in a text? We can use the `counter` class in Python, which utilizes dictionaries and another datatype, tuples. Let's walk through an example\n",
    "\n",
    "One of the most frequent tasks in computational text analysis is quickly summarizing the content of text. In this lesson we will learn how to summarze text by counting frequent words in the text. In the process we'll learn to think about features, which words are important, and we'll cover some common pre-processing steps. \n",
    "\n",
    "This techniques fits under the umbrella of Natural Language Processing, a term that incorporates many techiques and methods to process, analyze, and understand natural languages (as opposed to artificial languages like logics, or Python).\n",
    "\n",
    "\n",
    "## Outline:\n",
    "- Tokenizing Text and Type-Token Ratio\n",
    "    * Number of words\n",
    "    * Type-Token Ratio\n",
    "- Most frequent words\n",
    "- Pre-processing\n",
    "\n",
    "\n",
    "## Key Terms:\n",
    "\n",
    "* *stop words*: \n",
    "    * The most common words in a language.\n",
    "* *token*:\n",
    "    *  A token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing.\n",
    "* *type*:\n",
    "    * A type is the class of all tokens containing the same character sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's begin!\n",
    "\n",
    "First, we assign a sample sentence, our \"text\", to a variable called \"sentence\".\n",
    "\n",
    "Note: This sentence is a quote about what digital humanities means, from digital humanist Kathleen Fitzpatrick. Source: \"On Scholarly Communication and the Digital Humanities: An Interview with Kathleen Fitzpatrick\", *In the Library with the Lead Pipe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For me it has to do with the work that gets done at the crossroads of digital media and traditional humanistic study. And that happens in two different ways. On the one hand, it's bringing the tools and techniques of digital media to bear on traditional humanistic questions; on the other, it's also bringing humanistic modes of inquiry to bear on digital media.\n"
     ]
    }
   ],
   "source": [
    "#assign the desired sentence to the variable called 'sentence.'\n",
    "sentence = \"For me it has to do with the work that gets done at the crossroads of \\\n",
    "digital media and traditional humanistic study. And that happens in two different ways. \\\n",
    "On the one hand, it's bringing the tools and techniques of digital media to bear \\\n",
    "on traditional humanistic questions; on the other, it's also bringing humanistic modes \\\n",
    "of inquiry to bear on digital media.\"\n",
    "\n",
    "#print the content\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type-Token Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One quick calculation we can do on the text is determine it's type-token ratio.\n",
    "\n",
    "We know what a token is. But many tokens are repeated in a text. For example, in this sentence, the token \"the\" appears 5 times. \"The\" is a type. The 5 \"the\"s in the sentence are tokens. The TTR is simply the number of types divided by the number of tokens. A high TTR indicates a large amount of lexical variation or lexical diversity and a low TTR indicates relatively little lexical variation. The type-token ratio of speech, for example, is less than that of written language. \n",
    "\n",
    "To get a subset of our list that only contains one element of each type, we can use the `set` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['For',\n",
       " 'me',\n",
       " 'it',\n",
       " 'has',\n",
       " 'to',\n",
       " 'do',\n",
       " 'with',\n",
       " 'the',\n",
       " 'work',\n",
       " 'that',\n",
       " 'gets',\n",
       " 'done',\n",
       " 'at',\n",
       " 'the',\n",
       " 'crossroads',\n",
       " 'of',\n",
       " 'digital',\n",
       " 'media',\n",
       " 'and',\n",
       " 'traditional',\n",
       " 'humanistic',\n",
       " 'study.',\n",
       " 'And',\n",
       " 'that',\n",
       " 'happens',\n",
       " 'in',\n",
       " 'two',\n",
       " 'different',\n",
       " 'ways.',\n",
       " 'On',\n",
       " 'the',\n",
       " 'one',\n",
       " 'hand,',\n",
       " \"it's\",\n",
       " 'bringing',\n",
       " 'the',\n",
       " 'tools',\n",
       " 'and',\n",
       " 'techniques',\n",
       " 'of',\n",
       " 'digital',\n",
       " 'media',\n",
       " 'to',\n",
       " 'bear',\n",
       " 'on',\n",
       " 'traditional',\n",
       " 'humanistic',\n",
       " 'questions;',\n",
       " 'on',\n",
       " 'the',\n",
       " 'other,',\n",
       " \"it's\",\n",
       " 'also',\n",
       " 'bringing',\n",
       " 'humanistic',\n",
       " 'modes',\n",
       " 'of',\n",
       " 'inquiry',\n",
       " 'to',\n",
       " 'bear',\n",
       " 'on',\n",
       " 'digital',\n",
       " 'media.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list = sentence.split()\n",
    "sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'And',\n",
       " 'For',\n",
       " 'On',\n",
       " 'also',\n",
       " 'and',\n",
       " 'at',\n",
       " 'bear',\n",
       " 'bringing',\n",
       " 'crossroads',\n",
       " 'different',\n",
       " 'digital',\n",
       " 'do',\n",
       " 'done',\n",
       " 'gets',\n",
       " 'hand,',\n",
       " 'happens',\n",
       " 'has',\n",
       " 'humanistic',\n",
       " 'in',\n",
       " 'inquiry',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'me',\n",
       " 'media',\n",
       " 'media.',\n",
       " 'modes',\n",
       " 'of',\n",
       " 'on',\n",
       " 'one',\n",
       " 'other,',\n",
       " 'questions;',\n",
       " 'study.',\n",
       " 'techniques',\n",
       " 'that',\n",
       " 'the',\n",
       " 'to',\n",
       " 'tools',\n",
       " 'traditional',\n",
       " 'two',\n",
       " 'ways.',\n",
       " 'with',\n",
       " 'work'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type-token ratio\n",
    "\n",
    "len(set(sentence_list)) \n",
    "len(set(sentence_list)) / len(sentence_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Words\n",
    "\n",
    "We are often also interested in the most frequent words, which can help us quickly summarize a text. We can do this by looping through our sentence tokens variable and creating a counts dictionary.\n",
    "\n",
    "Let's walk through this code slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'For': 1,\n",
       " 'me': 1,\n",
       " 'it': 1,\n",
       " 'has': 1,\n",
       " 'to': 3,\n",
       " 'do': 1,\n",
       " 'with': 1,\n",
       " 'the': 5,\n",
       " 'work': 1,\n",
       " 'that': 2,\n",
       " 'gets': 1,\n",
       " 'done': 1,\n",
       " 'at': 1,\n",
       " 'crossroads': 1,\n",
       " 'of': 3,\n",
       " 'digital': 3,\n",
       " 'media': 2,\n",
       " 'and': 2,\n",
       " 'traditional': 2,\n",
       " 'humanistic': 3,\n",
       " 'study.': 1,\n",
       " 'And': 1,\n",
       " 'happens': 1,\n",
       " 'in': 1,\n",
       " 'two': 1,\n",
       " 'different': 1,\n",
       " 'ways.': 1,\n",
       " 'On': 1,\n",
       " 'one': 1,\n",
       " 'hand,': 1,\n",
       " \"it's\": 2,\n",
       " 'bringing': 2,\n",
       " 'tools': 1,\n",
       " 'techniques': 1,\n",
       " 'bear': 2,\n",
       " 'on': 3,\n",
       " 'questions;': 1,\n",
       " 'other,': 1,\n",
       " 'also': 1,\n",
       " 'modes': 1,\n",
       " 'inquiry': 1,\n",
       " 'media.': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = dict()\n",
    "for word in sentence_list:\n",
    "    if word not in counts:\n",
    "        counts[word] = 1\n",
    "    else:\n",
    "        counts[word] += 1\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can get the count (value) associated with any word (key)\n",
    "counts['humanistic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Frequent Words\n",
    "\n",
    "We'll have to creatively combine dictionaries and tuples to find the most frequent words in our sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary method .items() returns a list of tuples. This will eventually allow us to sort through the tuples.\n",
    "\n",
    "A `tuple` is a sequence of values much like a list. The values stored in a tuple can be any type, and they are indexed by integers. The important difference is that tuples are immutable. Tuples are also comparable and hashable so we can sort lists of them and use tuples as key values in Python dictionaries.\n",
    "\n",
    "Syntactically, a tuple is a comma-separated list of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('For', 1), ('me', 1), ('it', 1), ('has', 1), ('to', 3), ('do', 1), ('with', 1), ('the', 5), ('work', 1), ('that', 2), ('gets', 1), ('done', 1), ('at', 1), ('crossroads', 1), ('of', 3), ('digital', 3), ('media', 2), ('and', 2), ('traditional', 2), ('humanistic', 3), ('study.', 1), ('And', 1), ('happens', 1), ('in', 1), ('two', 1), ('different', 1), ('ways.', 1), ('On', 1), ('one', 1), ('hand,', 1), (\"it's\", 2), ('bringing', 2), ('tools', 1), ('techniques', 1), ('bear', 2), ('on', 3), ('questions;', 1), ('other,', 1), ('also', 1), ('modes', 1), ('inquiry', 1), ('media.', 1)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 1\n",
      "me 1\n",
      "it 1\n",
      "has 1\n",
      "to 3\n",
      "do 1\n",
      "with 1\n",
      "the 5\n",
      "work 1\n",
      "that 2\n",
      "gets 1\n",
      "done 1\n",
      "at 1\n",
      "crossroads 1\n",
      "of 3\n",
      "digital 3\n",
      "media 2\n",
      "and 2\n",
      "traditional 2\n",
      "humanistic 3\n",
      "study. 1\n",
      "And 1\n",
      "happens 1\n",
      "in 1\n",
      "two 1\n",
      "different 1\n",
      "ways. 1\n",
      "On 1\n",
      "one 1\n",
      "hand, 1\n",
      "it's 2\n",
      "bringing 2\n",
      "tools 1\n",
      "techniques 1\n",
      "bear 2\n",
      "on 3\n",
      "questions; 1\n",
      "other, 1\n",
      "also 1\n",
      "modes 1\n",
      "inquiry 1\n",
      "media. 1\n"
     ]
    }
   ],
   "source": [
    "#we can loop through these values like we might in a list, but notice the syntax here!\n",
    "for key, value in counts.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'For'),\n",
       " (1, 'me'),\n",
       " (1, 'it'),\n",
       " (1, 'has'),\n",
       " (3, 'to'),\n",
       " (1, 'do'),\n",
       " (1, 'with'),\n",
       " (5, 'the'),\n",
       " (1, 'work'),\n",
       " (2, 'that'),\n",
       " (1, 'gets'),\n",
       " (1, 'done'),\n",
       " (1, 'at'),\n",
       " (1, 'crossroads'),\n",
       " (3, 'of'),\n",
       " (3, 'digital'),\n",
       " (2, 'media'),\n",
       " (2, 'and'),\n",
       " (2, 'traditional'),\n",
       " (3, 'humanistic'),\n",
       " (1, 'study.'),\n",
       " (1, 'And'),\n",
       " (1, 'happens'),\n",
       " (1, 'in'),\n",
       " (1, 'two'),\n",
       " (1, 'different'),\n",
       " (1, 'ways.'),\n",
       " (1, 'On'),\n",
       " (1, 'one'),\n",
       " (1, 'hand,'),\n",
       " (2, \"it's\"),\n",
       " (2, 'bringing'),\n",
       " (1, 'tools'),\n",
       " (1, 'techniques'),\n",
       " (2, 'bear'),\n",
       " (3, 'on'),\n",
       " (1, 'questions;'),\n",
       " (1, 'other,'),\n",
       " (1, 'also'),\n",
       " (1, 'modes'),\n",
       " (1, 'inquiry'),\n",
       " (1, 'media.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_words = []\n",
    "\n",
    "for key, val in counts.items():\n",
    "    freq_words.append((val, key))\n",
    "\n",
    "freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 'the'),\n",
       " (3, 'to'),\n",
       " (3, 'on'),\n",
       " (3, 'of'),\n",
       " (3, 'humanistic'),\n",
       " (3, 'digital'),\n",
       " (2, 'traditional'),\n",
       " (2, 'that'),\n",
       " (2, 'media'),\n",
       " (2, \"it's\"),\n",
       " (2, 'bringing'),\n",
       " (2, 'bear'),\n",
       " (2, 'and'),\n",
       " (1, 'work'),\n",
       " (1, 'with'),\n",
       " (1, 'ways.'),\n",
       " (1, 'two'),\n",
       " (1, 'tools'),\n",
       " (1, 'techniques'),\n",
       " (1, 'study.'),\n",
       " (1, 'questions;'),\n",
       " (1, 'other,'),\n",
       " (1, 'one'),\n",
       " (1, 'modes'),\n",
       " (1, 'media.'),\n",
       " (1, 'me'),\n",
       " (1, 'it'),\n",
       " (1, 'inquiry'),\n",
       " (1, 'in'),\n",
       " (1, 'has'),\n",
       " (1, 'happens'),\n",
       " (1, 'hand,'),\n",
       " (1, 'gets'),\n",
       " (1, 'done'),\n",
       " (1, 'do'),\n",
       " (1, 'different'),\n",
       " (1, 'crossroads'),\n",
       " (1, 'at'),\n",
       " (1, 'also'),\n",
       " (1, 'On'),\n",
       " (1, 'For'),\n",
       " (1, 'And')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_words.sort(reverse=True)\n",
    "freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 the\n",
      "3 to\n",
      "3 on\n",
      "3 of\n",
      "3 humanistic\n",
      "3 digital\n",
      "2 traditional\n",
      "2 that\n",
      "2 media\n",
      "2 it's\n"
     ]
    }
   ],
   "source": [
    "for key, val in freq_words[:10]:\n",
    "    print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's save as a function!\n",
    "\n",
    "def word_counts(text_list):\n",
    "    counts = dict()\n",
    "    for word in text_list:\n",
    "        if word not in counts:\n",
    "            counts[word] = 1\n",
    "        else:\n",
    "            counts[word] += 1\n",
    "    \n",
    "    freq_words = []\n",
    "\n",
    "    for key, val in counts.items():\n",
    "        freq_words.append((val, key))\n",
    "    \n",
    "    freq_words.sort(reverse=True)\n",
    "    \n",
    "    return(freq_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Text and Preprocessing\n",
    "\n",
    "But what's the issue here? First, capitalization and punctuation are messing with our word counts. Second, the most frequent words, *the*, *to*, *on*, *of*, don't actually tell us much about the text. This is always the case. Stop words, or words that don't convey content, make up the vast majority of all text.\n",
    "\n",
    "Before doing any text analysis, we thus must do lots of preprocessing. The exact preprocessing steps you take will depend on what you're planning on doing. I'll go through common steps here, but think carefully about what steps you want to take when you do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for me it has to do with the work that gets done at the crossroads of digital media and traditional humanistic study. and that happens in two different ways. on the one hand, it's bringing the tools and techniques of digital media to bear on traditional humanistic questions; on the other, it's also bringing humanistic modes of inquiry to bear on digital media.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lowercase\n",
    "\n",
    "sentence_lc = sentence.lower()\n",
    "\n",
    "sentence_lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove punctuation\n",
    "#For punctuation use the list from the string library\n",
    "import string\n",
    "punct_list = string.punctuation\n",
    "\n",
    "punct_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for me it has to do with the work that gets done at the crossroads of digital media and traditional humanistic study and that happens in two different ways on the one hand its bringing the tools and techniques of digital media to bear on traditional humanistic questions on the other its also bringing humanistic modes of inquiry to bear on digital media'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_nopunct = ''.join([e for e in sentence_lc if e not in punct_list])\n",
    "sentence_nopunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#often, we want to remove stopwords\n",
    "\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', \n",
    "                     'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', \n",
    "                     'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', \n",
    "                     'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', \n",
    "                     'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', \n",
    "                     'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', \n",
    "                     'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', \n",
    "                     'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', \n",
    "                     'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', \n",
    "                     'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "                     'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 'can', 'will',\n",
    "                     'just', 'dont', 'should', 'aint', 'arent', 'couldn', 'could', 'would', 'much', 'must',\n",
    "                     'didnt', 'doesnt', 'hadnt', 'hasnt', 'havent', 'isnt', 'mightnt', 'mustnt', 'neednt', 'shan',\n",
    "                     'shouldnt', 'wasnt', 'werent', 'wont', 'wouldnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['work',\n",
       " 'gets',\n",
       " 'done',\n",
       " 'crossroads',\n",
       " 'digital',\n",
       " 'media',\n",
       " 'traditional',\n",
       " 'humanistic',\n",
       " 'study',\n",
       " 'happens',\n",
       " 'two',\n",
       " 'different',\n",
       " 'ways',\n",
       " 'one',\n",
       " 'hand',\n",
       " 'bringing',\n",
       " 'tools',\n",
       " 'techniques',\n",
       " 'digital',\n",
       " 'media',\n",
       " 'bear',\n",
       " 'traditional',\n",
       " 'humanistic',\n",
       " 'questions',\n",
       " 'also',\n",
       " 'bringing',\n",
       " 'humanistic',\n",
       " 'modes',\n",
       " 'inquiry',\n",
       " 'bear',\n",
       " 'digital',\n",
       " 'media']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokens = sentence_nopunct.split()\n",
    "sentence_clean = [word for word in sentence_tokens if word not in stop_words]\n",
    "sentence_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's save this as a function\n",
    "def word_tokenize(text):\n",
    "    text = text.lower()\n",
    "    text_clean = ''.join([e for e in text if e not in punct_list])\n",
    "    text_token =  text_clean.split()\n",
    "    text_token_clean = [word for word in text_token if word not in stop_words]\n",
    "    return text_token_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['work',\n",
       " 'gets',\n",
       " 'done',\n",
       " 'crossroads',\n",
       " 'digital',\n",
       " 'media',\n",
       " 'traditional',\n",
       " 'humanistic',\n",
       " 'study',\n",
       " 'happens',\n",
       " 'two',\n",
       " 'different',\n",
       " 'ways',\n",
       " 'one',\n",
       " 'hand',\n",
       " 'bringing',\n",
       " 'tools',\n",
       " 'techniques',\n",
       " 'digital',\n",
       " 'media',\n",
       " 'bear',\n",
       " 'traditional',\n",
       " 'humanistic',\n",
       " 'questions',\n",
       " 'also',\n",
       " 'bringing',\n",
       " 'humanistic',\n",
       " 'modes',\n",
       " 'inquiry',\n",
       " 'bear',\n",
       " 'digital',\n",
       " 'media']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#complete the line below\n",
    "sentence_tokens = word_tokenize(sentence)\n",
    "sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total number of words\n",
    "len(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 'media'),\n",
       " (3, 'humanistic'),\n",
       " (3, 'digital'),\n",
       " (2, 'traditional'),\n",
       " (2, 'bringing'),\n",
       " (2, 'bear'),\n",
       " (1, 'work'),\n",
       " (1, 'ways'),\n",
       " (1, 'two'),\n",
       " (1, 'tools'),\n",
       " (1, 'techniques'),\n",
       " (1, 'study'),\n",
       " (1, 'questions'),\n",
       " (1, 'one'),\n",
       " (1, 'modes'),\n",
       " (1, 'inquiry'),\n",
       " (1, 'happens'),\n",
       " (1, 'hand'),\n",
       " (1, 'gets'),\n",
       " (1, 'done'),\n",
       " (1, 'different'),\n",
       " (1, 'crossroads'),\n",
       " (1, 'also')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's count again! (remember our function)\n",
    "\n",
    "word_count = word_counts(sentence_tokens)\n",
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 'the'),\n",
       " (3, 'to'),\n",
       " (3, 'on'),\n",
       " (3, 'of'),\n",
       " (3, 'humanistic'),\n",
       " (3, 'digital'),\n",
       " (2, 'traditional'),\n",
       " (2, 'that'),\n",
       " (2, 'media'),\n",
       " (2, \"it's\"),\n",
       " (2, 'bringing'),\n",
       " (2, 'bear'),\n",
       " (2, 'and'),\n",
       " (1, 'work'),\n",
       " (1, 'with'),\n",
       " (1, 'ways.'),\n",
       " (1, 'two'),\n",
       " (1, 'tools'),\n",
       " (1, 'techniques'),\n",
       " (1, 'study.'),\n",
       " (1, 'questions;'),\n",
       " (1, 'other,'),\n",
       " (1, 'one'),\n",
       " (1, 'modes'),\n",
       " (1, 'media.'),\n",
       " (1, 'me'),\n",
       " (1, 'it'),\n",
       " (1, 'inquiry'),\n",
       " (1, 'in'),\n",
       " (1, 'has'),\n",
       " (1, 'happens'),\n",
       " (1, 'hand,'),\n",
       " (1, 'gets'),\n",
       " (1, 'done'),\n",
       " (1, 'do'),\n",
       " (1, 'different'),\n",
       " (1, 'crossroads'),\n",
       " (1, 'at'),\n",
       " (1, 'also'),\n",
       " (1, 'On'),\n",
       " (1, 'For'),\n",
       " (1, 'And')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reminder: word counts before preprocessing:\n",
    "\n",
    "old_count = word_counts(sentence.split())\n",
    "old_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "685139"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in a text file saved in your data folder\n",
    "\n",
    "with open(\"../data/Austen_PrideAndPrejudice.txt\", encoding='utf-8') as myfile:\n",
    "    #print(myfile)\n",
    "    mytext = myfile.read()\n",
    "\n",
    "mytext[:200]\n",
    "len(mytext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises!\n",
    "\n",
    "On the text we just read in, do the following. You are in control of what pre-processing steps you might want to take, if any. Use any of the functions we defined above if you want.\n",
    "\n",
    "1. How long is the text?\n",
    "2. Calculate the type-token ratio.\n",
    "3. Print the 20 most frequent words.\n",
    "4. How many short words are in the text? Short words == three characters or less.\n",
    "5. How many long words are in the text? Long words == seven characters or more.\n",
    "6. What is the (approximate) average word length in the text?\n",
    "7. What is the (approximate) average sentence length of the text?\n",
    "8. How long is the longest sentence?\n",
    "9. How long is the shortest sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121793"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. How long is the text?\n",
    "\n",
    "len(mytext.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10627047531467326"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Calculate the type-token ratio.\n",
    "\n",
    "len(set(mytext.split()))/len(mytext.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(772, 'mr'),\n",
       " (585, 'elizabeth'),\n",
       " (396, 'said'),\n",
       " (365, 'darcy'),\n",
       " (342, 'mrs'),\n",
       " (292, 'bennet'),\n",
       " (288, 'one'),\n",
       " (283, 'every'),\n",
       " (281, 'miss'),\n",
       " (258, 'jane'),\n",
       " (253, 'bingley'),\n",
       " (235, 'know'),\n",
       " (222, 'though'),\n",
       " (221, 'never'),\n",
       " (218, 'soon'),\n",
       " (212, 'well'),\n",
       " (211, 'think'),\n",
       " (210, 'now'),\n",
       " (201, 'time'),\n",
       " (200, 'might')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Print the 20 most frequent words.\n",
    "\n",
    "mytext_tokens = word_tokenize(mytext)\n",
    "austen_wc = word_counts(mytext_tokens)\n",
    "austen_wc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52447"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. How many short words are in the text? Short words == three characters or less.\n",
    "\n",
    "len([x for x in mytext.split() if len(x)<=3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23066\n"
     ]
    }
   ],
   "source": [
    "# 5. How many long words are in the text? Long words == seven characters or more.\n",
    "\n",
    "long_words = 0\n",
    "\n",
    "for e in mytext_tokens:\n",
    "    if len(e)>=7:\n",
    "        long_words = long_words + 1\n",
    "print(long_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.725953783795077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.625438243577217"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6. What is the (approximate) average word length in the text?\n",
    "\n",
    "print(len(mytext) / mytext.count(' '))\n",
    "\n",
    "len(mytext) / len(mytext.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.41136543014996"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7. What is the (approximate) average sentence length of the text?\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sentences = mytext.split('.')\n",
    "np.mean([len(x.split()) for x in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.41136543014996"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#or, without numpy:\n",
    "\n",
    "sentences = mytext.split('.')\n",
    "sum([len(x.split()) for x in sentences]) / len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 3, 7, 2, 1, 3, 2, 23, 48, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. How long is the longest sentence?\n",
    "\n",
    "print([len(x.split()) for x in sentences][:10])\n",
    "max([len(x.split()) for x in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9. How long is the shortest sentence?\n",
    "min([len(x.split()) for x in sentences])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
